{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of regions on the taxi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from moc.conformal import *\n",
    "from moc.models.mqf2.lightning_module import MQF2LightningModule\n",
    "from moc.models.trainers.lightning_trainer import get_lightning_trainer\n",
    "from moc.configs.config import get_config\n",
    "from moc.utils.run_config import RunConfig\n",
    "from moc.analysis.dataframes import load_datamodule\n",
    "from moc.metrics.metrics_computer import compute_cum_region_size\n",
    "from moc.metrics.cache import Cache, EmptyCache\n",
    "\n",
    "import taxi_example.taxi_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()\n",
    "# Uncomment this line for to train the model only on the first batches\n",
    "config.fast = False\n",
    "# config.device = 'cpu'\n",
    "config.default_batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'taxi'\n",
    "rc = RunConfig(config, 'wang', dataset, 0, hparams={})\n",
    "datamodule = load_datamodule(rc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_X, scaled_test_Y = datamodule.data_test[:]\n",
    "\n",
    "test_X = datamodule.scaler_x.inverse_transform(scaled_test_X)\n",
    "test_Y = datamodule.scaler_y.inverse_transform(scaled_test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "p, q = datamodule.input_dim, datamodule.output_dim\n",
    "mqf2_model = MQF2LightningModule(p, q)\n",
    "trainer = get_lightning_trainer(rc)\n",
    "trainer.fit(mqf2_model, datamodule)\n",
    "mqf2_model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformalizers = {\n",
    "    'M-CP': M_CP,\n",
    "    'CopulaCPTS': CopulaCPTS,\n",
    "    'DR-CP': DR_CP,\n",
    "    'C-HDR': C_HDR,\n",
    "    'PCP': PCP,\n",
    "    'HD-PCP': HD_PCP,\n",
    "    'STDQR': STDQR,\n",
    "    'C-PCP': C_PCP,\n",
    "    'L-CP': L_CP,\n",
    "}\n",
    "\n",
    "alpha = 0.2\n",
    "n_shift = 2\n",
    "\n",
    "x, y = datamodule.data_calib[:]\n",
    "xlim = x[:, 0].min() * n_shift, x[:, 0].max() * n_shift\n",
    "ylim = y[:, 0].min() * n_shift, y[:, 0].max() * n_shift\n",
    "zlim = y[:, 1].min() * n_shift, y[:, 1].max() * n_shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "batch_size_test = 1000\n",
    "\n",
    "cache_calib = Cache(mqf2_model, datamodule.calib_dataloader(), n_samples, add_second_sample=True)\n",
    "cache_test = Cache(mqf2_model, datamodule.get_dataloader(datamodule.data_test, batch_size=batch_size_test), n_samples, add_second_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_test_for_sample(idx: int, cache):\n",
    "    batch = idx // 1000\n",
    "    pos_in_batch = idx % 1000\n",
    "\n",
    "    current_cache_test_for_sample = cache[batch].copy()\n",
    "    current_cache_test_for_sample['samples'] = current_cache_test_for_sample['samples'][:, pos_in_batch:pos_in_batch+1, :]\n",
    "    current_cache_test_for_sample['log_probs'] = current_cache_test_for_sample['log_probs'][:, pos_in_batch:pos_in_batch+1]\n",
    "    current_cache_test_for_sample['samples2'] = current_cache_test_for_sample['samples2'][:, pos_in_batch:pos_in_batch+1, :]\n",
    "    current_cache_test_for_sample['log_probs2'] = current_cache_test_for_sample['log_probs2'][:, pos_in_batch:pos_in_batch+1]\n",
    "\n",
    "    return current_cache_test_for_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get uncertainty for X\n",
    "conformal_method_unc = C_HDR(datamodule.calib_dataloader(), mqf2_model, cache_calib=cache_calib)\n",
    "log_region_sizes = []\n",
    "\n",
    "for current_idx in tqdm(range(len(test_X[:100, 0]))):\n",
    "    current_x = scaled_test_X[current_idx].unsqueeze(0)\n",
    "    associated_y = scaled_test_Y[current_idx].unsqueeze(0)\n",
    "\n",
    "    current_cache_test = get_cache_test_for_sample(current_idx, cache_test)\n",
    "\n",
    "    if conformal_method_unc.is_in_region(current_x, associated_y, alpha=0.2, cache=current_cache_test):\n",
    "        region_size = compute_cum_region_size(conformal_method_unc, mqf2_model, alpha, current_x, n_samples=100, cache_test=current_cache_test)[-1].item()\n",
    "        if 0 < region_size < 1E308:\n",
    "            log_region_sizes.append(region_size)\n",
    "        else:\n",
    "            log_region_sizes.append(np.mean(log_region_sizes))\n",
    "    else:\n",
    "        log_region_sizes.append(np.mean(log_region_sizes))\n",
    "\n",
    "low_uncertainty_idx = np.argmin(log_region_sizes)\n",
    "high_uncertainty_idx = np.argmax(log_region_sizes)\n",
    "low_uncertainty_idx, high_uncertainty_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_indices = [96, 88]\n",
    "alpha = 0.2\n",
    "\n",
    "for idx in point_indices:\n",
    "    scaled_x_test, scaled_y_test = scaled_test_X[idx].unsqueeze(0).to(config.device), scaled_test_Y[idx].unsqueeze(0).to(config.device)\n",
    "    x_test, y_test = test_X[idx], test_Y[idx]\n",
    "\n",
    "    current_cache_test = get_cache_test_for_sample(idx, cache_test)\n",
    "\n",
    "    visual_group_X, visual_group_Y = test_X[idx:idx+5], test_Y[idx:idx+5]\n",
    "    utils.visualize_data_on_map(visual_group_X.numpy(), visual_group_Y.numpy(), idx)\n",
    "\n",
    "    for name, conformalizer in tqdm(conformalizers.items()):\n",
    "        method = conformalizer(datamodule.calib_dataloader(), mqf2_model, cache_calib=cache_calib)\n",
    "        scaled_contour = utils.get_contour(scaled_x_test, method, alpha, xlim, zlim, grid_side=3000, cache=current_cache_test)\n",
    "        region_size = compute_cum_region_size(method, mqf2_model, alpha, scaled_x_test, n_samples=1000, cache_test=current_cache_test)[-1].item()\n",
    "        \n",
    "        utils.show_contours_on_map(name, x_test, y_test, scaled_contour, idx, region_size, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
